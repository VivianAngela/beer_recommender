{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import pickle\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(x):\n",
    "    x = str(x)\n",
    "    return x.translate(string.maketrans('',''),string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beers = pd.read_pickle('all_beer_reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#beers = beers[beers.num_reviews >= 10]\n",
    "beers.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20470, 20)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [review for review in beers.reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [' '.join(review) for review in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [remove_punctuation(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [review.lower() for review in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [re.sub(r'[0-9]','',doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoplist = set('thanks an one little just has be up had no with is this it i but that on not very some as was like from its bit at more into there my pours for a of the and to in'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add brewery acronyms to stoplist\n",
    "brewery_acronyms = set('fff rr br gi pte '.split())\n",
    "stoplist = stoplist.union(brewery_acronyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add brewery names to stoplist\n",
    "brewery_words = []\n",
    "for brewery in beers.brewery_name:\n",
    "    for word in brewery.lower().split():\n",
    "        brewery_words.append(remove_punctuation(word.encode('utf-8')))\n",
    "    \n",
    "brewery_words = set(brewery_words)\n",
    "stoplist = stoplist.union(brewery_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add weird beer name words to stoplist, like stoopid and heady etc. pliny, shit like that\n",
    "beer_name_words = []\n",
    "for beer in beers.name:\n",
    "    for word in beer.lower().split():\n",
    "        beer_name_words.append(remove_punctuation(word.encode('utf-8')))\n",
    "        \n",
    "beer_name_frequency = defaultdict(int)\n",
    "for word in beer_name_words:\n",
    "    beer_name_frequency[word] += 1\n",
    "    \n",
    "sorted(beer_name_frequency.items(), key = lambda x: -x[1])\n",
    "beer_name_words = [word for word in beer_name_words if beer_name_frequency[word] < 9]\n",
    "beer_name_words = set(beer_name_words)\n",
    "stoplist = stoplist.union(beer_name_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split() if word not in stoplist and len(word) > 2]for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "texts = [[token for token in text if frequency[token] > 50]for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_input = 'Heady Topper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abrasive Ale\t:\t96.61\n",
      "Ephraim\t:\t96.26\n",
      "Abner\t:\t96.07\n",
      "Otter Creek Brewing / Lawson's Double Dose IPA\t:\t95.97\n",
      "Stone Enjoy By IPA\t:\t95.97\n"
     ]
    }
   ],
   "source": [
    "# get the reviews for a beer\n",
    "beer_name_inputted = 1\n",
    "try:\n",
    "    doc= documents[beers[beers.name == text_input].index[0]]\n",
    "except IndexError:\n",
    "    print 'Beer Name Not Inputted'\n",
    "    doc = text_input\n",
    "    beer_name_inputted = 0\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "\n",
    "sims = index[vec_lsi]\n",
    "similar_beers = []\n",
    "for beer in sorted(enumerate(sims), key = lambda x: -x[1])[beer_name_inputted:beer_name_inputted+5]:\n",
    "    similar_beers.append(beer[0])\n",
    "    print(beers.name.iloc[beer[0]] + '\\t:\\t%.2f' % (beer[1]*100))\n",
    "similar_beers = beers.iloc[similar_beers,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_beer_keywords = []\n",
    "for item in sorted(corpus_tfidf[beers[beers.name == text_input].index[0]], key = lambda x: -x[1])[:5]:\n",
    "    if frequency[dictionary[item[0]]] > 50:\n",
    "        input_beer_keywords.append(dictionary[item[0]])\n",
    "\n",
    "similar_beer_words = []\n",
    "for beer in list(similar_beers.index):\n",
    "    similar_beer_words.append([dictionary[item[0]] for item in\n",
    "                               sorted(corpus_tfidf[beer], key = lambda x: -x[1])[:5]\n",
    "                               if dictionary[item[0]] in input_beer_keywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'grapefruit', u'dipa', u'tropical', u'pineapple', u'mango']\n",
      "[[u'dipa', u'tropical', u'grapefruit', u'pineapple'], [u'dipa', u'grapefruit', u'tropical', u'mango', u'pineapple'], [u'dipa', u'grapefruit', u'mango', u'tropical'], [u'dipa', u'tropical', u'grapefruit', u'mango'], [u'tropical', u'grapefruit', u'pineapple', u'mango']]\n"
     ]
    }
   ],
   "source": [
    "print input_beer_keywords\n",
    "print similar_beer_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([u'grapefruit', u'dipa', u'tropical', u'pineapple', u'mango'],\n",
       " [[u'dipa', u'tropical', u'grapefruit', u'pineapple'],\n",
       "  [u'dipa', u'grapefruit', u'tropical', u'mango', u'pineapple'],\n",
       "  [u'dipa', u'grapefruit', u'mango', u'tropical'],\n",
       "  [u'dipa', u'tropical', u'grapefruit', u'mango'],\n",
       "  [u'tropical', u'grapefruit', u'pineapple', u'mango']])"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_beer_keywords(text_input):\n",
    "    input_beer_keywords = []\n",
    "    for item in sorted(corpus_tfidf[beers[beers.name == text_input].index[0]], key = lambda x: -x[1])[:5]:\n",
    "        input_beer_keywords.append(dictionary[item[0]])\n",
    "\n",
    "    similar_beer_words = []\n",
    "    for beer in list(similar_beers.index):\n",
    "        similar_beer_words.append([dictionary[item[0]] for item in sorted(corpus_tfidf[beer], key = lambda x: -x[1])[:5] if dictionary[item[0]] in input_beer_keywords])\n",
    "    return (input_beer_keywords, similar_beer_words)\n",
    "\n",
    "get_beer_keywords(text_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(documents,open('flask/app/models/documents.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(dictionary,open('flask/app/models/dictionary.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(lsi,open('flask/app/models/lsi.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(corpus,open('flask/app/models/corpus.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(index,open('flask/app/models/index.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beers.to_pickle('flask/app/models/beer_review_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(corpus_tfidf,open('flask/app/models/tfidf.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-458-27d1319113a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mSYMBOLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"-----\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"---\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"“\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"”\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'ve\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mSTOPLIST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"n't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ca\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"’m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'re\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n/a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%hesitation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nnj'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dnmt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'think'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yeah'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenizeText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "import textblob\n",
    "# The fuck is this shit b\n",
    "\n",
    "# Kens lemmatizer, hm\n",
    "\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\", '..']\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\", \"’m\", '..', \"'re\", 'n/a', '%hesitation', 'nnj', 'dnmt', 'think', 'yeah'] + list(STOPWORDS)) \n",
    "\n",
    "def tokenizeText(sample):\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizeText, max_features= 1000, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
